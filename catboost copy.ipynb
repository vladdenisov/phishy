{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "import warnings\n",
    "from sklearn.metrics import classification_report\n",
    "import shap\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Union, Tuple, Optional\n",
    "import numpy as np\n",
    "import re\n",
    "import logging\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import shap\n",
    "from urllib.parse import urlparse\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "class HTMLPhishingDetector:\n",
    "    def __init__(\n",
    "        self,\n",
    "        iterations: int = 1000,\n",
    "        learning_rate: float = 0.1,\n",
    "        random_state: int = 42,\n",
    "        model_path: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the phishing detector with enhanced configuration.\n",
    "        \n",
    "        Args:\n",
    "            iterations: Number of boosting iterations (default: 1000)\n",
    "            learning_rate: Learning rate for gradient descent (default: 0.1)\n",
    "            random_state: Random seed for reproducibility (default: 42)\n",
    "            model_path: Path to load a pre-trained model (default: None)\n",
    "        \"\"\"\n",
    "        self.model = CatBoostClassifier(\n",
    "            iterations=iterations,\n",
    "            learning_rate=learning_rate,\n",
    "            random_seed=random_state,\n",
    "            verbose=True,\n",
    "            eval_metric='AUC',\n",
    "            loss_function='Logloss',\n",
    "            early_stopping_rounds=50,  # Added early stopping\n",
    "            max_depth=6,  # Control tree depth\n",
    "            l2_leaf_reg=3,  # L2 regularization\n",
    "            border_count=128  # Increase feature discretization\n",
    "        )\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_names: List[str] = []\n",
    "        self.explainer = None\n",
    "        self._initialize_logging()\n",
    "        \n",
    "        # Load pre-trained model if provided\n",
    "        if model_path:\n",
    "            self.load_model(model_path)\n",
    "            \n",
    "        # Initialize common patterns\n",
    "        self._initialize_patterns()\n",
    "        \n",
    "        # Load known phishing patterns\n",
    "        self._load_phishing_patterns()\n",
    "\n",
    "    def _initialize_logging(self):\n",
    "        \"\"\"Initialize enhanced logging configuration.\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.StreamHandler(),\n",
    "                logging.FileHandler('phishing_detector.log')\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def _initialize_patterns(self):\n",
    "        \"\"\"Initialize regex patterns and common indicators.\"\"\"\n",
    "        self.patterns = {\n",
    "            'ip_address': re.compile(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b'),\n",
    "            'data_uri': re.compile(r'data:(?:[^;]*;)*(?:base64,)?[a-zA-Z0-9+/]+={0,2}'),\n",
    "            'obfuscated_js': re.compile(\n",
    "                r'eval\\(|String\\.fromCharCode|unescape\\(|escape\\(|atob\\(|btoa\\('\n",
    "                r'|decodeURIComponent\\(|encodeURIComponent\\(|debugger|'\n",
    "                r'\\\\x[0-9a-fA-F]{2}|\\\\u[0-9a-fA-F]{4}|\\\\[0-7]{3}'\n",
    "            ),\n",
    "            'suspicious_functions': re.compile(\n",
    "                r'document\\.write|document\\.location|window\\.location|'\n",
    "                r'document\\.cookie|localStorage|sessionStorage'\n",
    "            ),\n",
    "            'urgent_words': re.compile(\n",
    "                r'urgent|immediate|verify|suspend|restrict|limit|cancel|account|'\n",
    "                r'security|unauthorized|login|sign.?in|expires?|password|'\n",
    "                r'credentials|confirm|update|validate|required|important'\n",
    "            )\n",
    "        }\n",
    "\n",
    "    def _load_phishing_patterns(self):\n",
    "        \"\"\"Load known phishing patterns and signatures.\"\"\"\n",
    "        # This could be expanded to load from a database or file\n",
    "        self.known_patterns = {\n",
    "            'fake_domains': [\n",
    "                'paypa1.com', 'amaz0n.com', 'g00gle.com',\n",
    "                'faceb00k.com', 'twltter.com'\n",
    "            ],\n",
    "            'suspicious_tlds': [\n",
    "                '.tk', '.ml', '.ga', '.cf', '.gq', '.xyz'\n",
    "            ],\n",
    "            'common_redirects': [\n",
    "                'url=', 'redirect=', 'return=', 'redir=', 'r=', 'u='\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    def extract_html_features(self, html_content: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Extract enhanced features from HTML content for phishing detection.\n",
    "        \n",
    "        Args:\n",
    "            html_content: Raw HTML string\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Array of extracted feature values\n",
    "        \"\"\"\n",
    "        if not html_content or not isinstance(html_content, str):\n",
    "            raise ValueError(\"HTML content must be a non-empty string\")\n",
    "            \n",
    "        try:\n",
    "            features = {}\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            \n",
    "            # Basic form and input analysis\n",
    "            features.update(self._extract_form_features(soup))\n",
    "            \n",
    "            # Enhanced link analysis\n",
    "            features.update(self._extract_link_features(soup))\n",
    "            \n",
    "            # Script and resource analysis\n",
    "            features.update(self._extract_script_features(soup))\n",
    "            \n",
    "            # Content and text analysis\n",
    "            features.update(self._extract_content_features(soup))\n",
    "            \n",
    "            # Security indicators\n",
    "            features.update(self._extract_security_features(soup))\n",
    "            \n",
    "            # Technical indicators\n",
    "            features.update(self._extract_technical_features(soup, html_content))\n",
    "            \n",
    "            # Normalize features if needed\n",
    "            features = self._normalize_features(features)\n",
    "            \n",
    "            return np.array(list(features.values()))\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error extracting features: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _extract_form_features(self, soup: BeautifulSoup) -> Dict[str, float]:\n",
    "        \"\"\"Extract enhanced form-related features.\"\"\"\n",
    "        features = {}\n",
    "        forms = soup.find_all('form')\n",
    "        \n",
    "        features['form_count'] = len(forms)\n",
    "        features['password_fields'] = len(soup.find_all('input', {'type': 'password'}))\n",
    "        \n",
    "        # Analyze form actions\n",
    "        external_actions = 0\n",
    "        suspicious_actions = 0\n",
    "        for form in forms:\n",
    "            action = form.get('action', '').lower()\n",
    "            if action.startswith(('http', '//')):\n",
    "                external_actions += 1\n",
    "\n",
    "                    \n",
    "        features['external_form_actions'] = external_actions\n",
    "        features['suspicious_form_actions'] = suspicious_actions\n",
    "        \n",
    "        # Enhanced input field analysis\n",
    "        input_fields = soup.find_all('input')\n",
    "        features['input_field_count'] = len(input_fields)\n",
    "        \n",
    "        sensitive_types = {\n",
    "            'password', 'email', 'tel', 'credit-card', 'card-number', \n",
    "            'ssn', 'social-security', 'bank-account', 'routing-number'\n",
    "        }\n",
    "        \n",
    "        sensitive_count = 0\n",
    "        suspicious_names = 0\n",
    "        for field in input_fields:\n",
    "            field_type = field.get('type', '').lower()\n",
    "            field_name = field.get('name', '').lower()\n",
    "            \n",
    "            if (field_type in sensitive_types or \n",
    "                any(term in field_name for term in sensitive_types)):\n",
    "                sensitive_count += 1\n",
    "                \n",
    "            if self._is_suspicious_input_name(field_name):\n",
    "                suspicious_names += 1\n",
    "                \n",
    "        features['sensitive_input_count'] = sensitive_count\n",
    "        features['suspicious_input_names'] = suspicious_names\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def _extract_link_features(self, soup: BeautifulSoup) -> Dict[str, float]:\n",
    "        \"\"\"Extract enhanced link-related features.\"\"\"\n",
    "        features = {}\n",
    "        links = soup.find_all('a')\n",
    "        features['link_count'] = len(links)\n",
    "        \n",
    "        external_links = 0\n",
    "        internal_links = 0\n",
    "        suspicious_links = 0\n",
    "        redirect_links = 0\n",
    "        same_text_diff_targets = 0\n",
    "        \n",
    "        # Track unique domains and their frequencies\n",
    "        domains = {}\n",
    "        link_texts = {}\n",
    "        \n",
    "        for link in links:\n",
    "            href = link.get('href', '').strip()\n",
    "            text = link.get_text().strip()\n",
    "            \n",
    "            if not href:\n",
    "                continue\n",
    "                \n",
    "            # Analyze link target\n",
    "            if href.startswith(('http', '//', 'www.')):\n",
    "                external_links += 1\n",
    "                    \n",
    "                if self._contains_redirect(href):\n",
    "                    redirect_links += 1\n",
    "                    \n",
    "            elif href and not href.startswith('#'):\n",
    "                internal_links += 1\n",
    "                \n",
    "            # Analyze link text vs target mismatch\n",
    "            if text:\n",
    "                if text in link_texts:\n",
    "                    if href != link_texts[text]:\n",
    "                        same_text_diff_targets += 1\n",
    "                else:\n",
    "                    link_texts[text] = href\n",
    "                    \n",
    "        total_links = external_links + internal_links\n",
    "        \n",
    "        features.update({\n",
    "            'external_links_ratio': external_links / (total_links + 1),\n",
    "            'suspicious_link_ratio': suspicious_links / (total_links + 1),\n",
    "            'redirect_link_ratio': redirect_links / (total_links + 1),\n",
    "            'domain_diversity': len(domains) / (total_links + 1),\n",
    "            'max_domain_concentration': max(domains.values()) / (total_links + 1) if domains else 0,\n",
    "            'same_text_diff_targets': same_text_diff_targets\n",
    "        })\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def _extract_script_features(self, soup: BeautifulSoup) -> Dict[str, float]:\n",
    "        \"\"\"Extract enhanced script-related features.\"\"\"\n",
    "        features = {}\n",
    "        scripts = soup.find_all('script')\n",
    "        \n",
    "        features['script_count'] = len(scripts)\n",
    "        features['external_scripts'] = sum(1 for script in scripts \n",
    "                                         if script.get('src', '').startswith(('http', '//')))\n",
    "        \n",
    "        # Analyze script content\n",
    "        obfuscated_count = 0\n",
    "        suspicious_functions = 0\n",
    "        encoded_strings = 0\n",
    "        \n",
    "        for script in scripts:\n",
    "            content = script.string if script.string else ''\n",
    "            if not content:\n",
    "                continue\n",
    "                \n",
    "            if self.patterns['obfuscated_js'].search(content):\n",
    "                obfuscated_count += 1\n",
    "                \n",
    "            if self.patterns['suspicious_functions'].search(content):\n",
    "                suspicious_functions += 1\n",
    "                \n",
    "            # Check for long encoded strings\n",
    "            encoded_strings += len(re.findall(r'[\\w+/]{50,}={0,2}', content))\n",
    "            \n",
    "        features.update({\n",
    "            'obfuscated_scripts': obfuscated_count,\n",
    "            'suspicious_functions': suspicious_functions,\n",
    "            'encoded_strings': encoded_strings\n",
    "        })\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def _extract_content_features(self, soup: BeautifulSoup) -> Dict[str, float]:\n",
    "        \"\"\"Extract enhanced content-related features.\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Get all text content\n",
    "        text_content = ' '.join(soup.stripped_strings).lower()\n",
    "        features['text_length'] = len(text_content)\n",
    "        \n",
    "        # Analyze text characteristics\n",
    "        features.update({\n",
    "            'urgent_word_count': len(self.patterns['urgent_words'].findall(text_content)),\n",
    "            'suspicious_character_ratio': self._calculate_suspicious_chars(text_content),\n",
    "            'language_mix_score': self._calculate_language_mix(text_content)\n",
    "        })\n",
    "        \n",
    "        # Hidden elements analysis\n",
    "        hidden_elements = soup.find_all(\n",
    "            ['div', 'span', 'input', 'a'], \n",
    "            style=re.compile(r'display\\s*:\\s*none|visibility\\s*:\\s*hidden', re.I)\n",
    "        )\n",
    "        features['hidden_element_count'] = len(hidden_elements)\n",
    "        \n",
    "        # Analyze element positioning\n",
    "        features['suspicious_positioning'] = self._analyze_element_positioning(soup)\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def _extract_security_features(self, soup: BeautifulSoup) -> Dict[str, float]:\n",
    "        \"\"\"Extract enhanced security-related features.\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # SSL/HTTPS analysis\n",
    "        features['has_https_link'] = int(bool(soup.find('a', href=re.compile(r'^https://'))))\n",
    "        \n",
    "        # Security headers and meta tags\n",
    "        meta_tags = soup.find_all('meta')\n",
    "        features['meta_tag_count'] = len(meta_tags)\n",
    "        \n",
    "        security_headers = {\n",
    "            'content-security-policy': 0,\n",
    "            'x-frame-options': 0,\n",
    "            'x-xss-protection': 0\n",
    "        }\n",
    "        \n",
    "        for tag in meta_tags:\n",
    "            http_equiv = tag.get('http-equiv', '').lower()\n",
    "            if http_equiv in security_headers:\n",
    "                security_headers[http_equiv] = 1\n",
    "                \n",
    "        features.update(security_headers)\n",
    "        \n",
    "        # Favicon analysis\n",
    "        features['has_favicon'] = int(bool(\n",
    "            soup.find('link', rel=re.compile(r'^(shortcut\\s+)?icon$', re.I))\n",
    "        ))\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def _extract_technical_features(self, soup: BeautifulSoup, html_content: str) -> Dict[str, float]:\n",
    "        \"\"\"Extract enhanced technical features.\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # HTML structure analysis\n",
    "        features['tag_density'] = len(soup.find_all(True)) / (len(html_content) + 1)\n",
    "        features['max_depth'] = self._calculate_max_depth(soup)\n",
    "        \n",
    "        # Resource loading analysis\n",
    "        resources = soup.find_all(['img', 'script', 'link', 'iframe'])\n",
    "        features['resource_count'] = len(resources)\n",
    "        \n",
    "        external_resources = 0\n",
    "        data_uri_resources = 0\n",
    "        for resource in resources:\n",
    "            src = resource.get('src', '') or resource.get('href', '')\n",
    "            if src.startswith(('http', '//')):\n",
    "                external_resources += 1\n",
    "            if src.startswith('data:'):\n",
    "                data_uri_resources += 1\n",
    "                \n",
    "        features['external_resource_ratio'] = external_resources / (len(resources) + 1)\n",
    "        features['data_uri_ratio'] = data_uri_resources / (len(resources) + 1)\n",
    "        \n",
    "        # Code characteristics\n",
    "        features['html_length'] = len(html_content)\n",
    "        features['comments_length'] = sum(len(str(comment)) for comment in soup.find_all(string=lambda text: isinstance(text, str)))\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def _normalize_features(self, features: Dict[str, float]) -> Dict[str, float]:\n",
    "        \"\"\"Normalize feature values to appropriate ranges.\"\"\"\n",
    "        # Ensure all values are numeric\n",
    "        for key, value in features.items():\n",
    "            if isinstance(value, bool):\n",
    "                features[key] = float(value)\n",
    "            elif not isinstance(value, (int, float)):\n",
    "                features[key] = float(value)\n",
    "                \n",
    "        # Cap extreme values\n",
    "        for key, value in features.items():\n",
    "            if key.endswith('_ratio'):\n",
    "                features[key] = min(max(value, 0.0), 1.0)\n",
    "            elif key.endswith('_count'):\n",
    "                features[key] = min(value, 1000)  # Cap counts at reasonable maximum\n",
    "                \n",
    "        return features\n",
    "\n",
    "\n",
    "    def _contains_redirect(self, url: str) -> bool:\n",
    "        \"\"\"Check if URL contains redirect patterns.\"\"\"\n",
    "        try:\n",
    "            parsed = urlparse(url)\n",
    "            query = parsed.query.lower()\n",
    "            \n",
    "            return any(pattern in query for pattern in self.known_patterns['common_redirects'])\n",
    "            \n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    def _is_suspicious_input_name(self, name: str) -> bool:\n",
    "        \"\"\"Check if input field name is suspicious.\"\"\"\n",
    "        suspicious_patterns = [\n",
    "            r'(?:card|cc).*num',\n",
    "            r'(?:cvv|cvc|ccv)',\n",
    "            r'ssn',\n",
    "            r'(?:bank|account).*(?:num|no)',\n",
    "            r'routing',\n",
    "            r'swift',\n",
    "            r'passw(?:or)?d',\n",
    "        ]\n",
    "        \n",
    "        return any(re.search(pattern, name, re.I) for pattern in suspicious_patterns)\n",
    "\n",
    "    def _calculate_suspicious_chars(self, text: str) -> float:\n",
    "        \"\"\"Calculate ratio of suspicious characters in text.\"\"\"\n",
    "        suspicious_chars = re.findall(r'[^\\x00-\\x7F]|[\\\\/<>]', text)\n",
    "        return len(suspicious_chars) / (len(text) + 1)\n",
    "\n",
    "    def _calculate_language_mix(self, text: str) -> float:\n",
    "        \"\"\"Calculate score for mixed language usage.\"\"\"\n",
    "        # Simple heuristic for mixed scripts\n",
    "        scripts = {\n",
    "            'latin': len(re.findall(r'[a-zA-Z]', text)),\n",
    "            'cyrillic': len(re.findall(r'[а-яА-Я]', text)),\n",
    "            'chinese': len(re.findall(r'[\\u4e00-\\u9fff]', text)),\n",
    "            'arabic': len(re.findall(r'[\\u0600-\\u06ff]', text)),\n",
    "        }\n",
    "        \n",
    "        total_chars = sum(scripts.values())\n",
    "        if total_chars == 0:\n",
    "            return 0\n",
    "            \n",
    "        # Calculate entropy-like measure of script mixing\n",
    "        proportions = [count / total_chars for count in scripts.values() if count > 0]\n",
    "        return -sum(p * np.log(p) for p in proportions)\n",
    "\n",
    "    def _analyze_element_positioning(self, soup: BeautifulSoup) -> int:\n",
    "        \"\"\"Analyze suspicious element positioning.\"\"\"\n",
    "        suspicious_count = 0\n",
    "        \n",
    "        # Check for elements positioned outside viewport\n",
    "        positioned_elements = soup.find_all(style=re.compile(\n",
    "            r'position\\s*:\\s*absolute|position\\s*:\\s*fixed'\n",
    "        ))\n",
    "        \n",
    "        for element in positioned_elements:\n",
    "            style = element.get('style', '')\n",
    "            # Check for negative positioning or very large values\n",
    "            if re.search(r'(?:top|left|right|bottom)\\s*:\\s*-?\\d{4,}px', style):\n",
    "                suspicious_count += 1\n",
    "            # Check for positioning far outside normal viewport\n",
    "            if re.search(r'(?:top|left|right|bottom)\\s*:\\s*[1-9]\\d{3,}px', style):\n",
    "                suspicious_count += 1\n",
    "                \n",
    "        return suspicious_count\n",
    "\n",
    "    def _calculate_max_depth(self, soup: BeautifulSoup) -> int:\n",
    "        \"\"\"Calculate maximum nesting depth of HTML elements.\"\"\"\n",
    "        max_depth = 0\n",
    "        \n",
    "        for element in soup.find_all(True):\n",
    "            depth = len(list(element.parents))\n",
    "            max_depth = max(max_depth, depth)\n",
    "            \n",
    "        return max_depth\n",
    "\n",
    "    def save_model(self, path: str):\n",
    "        \"\"\"Save the trained model and associated data.\"\"\"\n",
    "        model_data = {\n",
    "            'model': self.model.save_model(),\n",
    "            'scaler': self.scaler,\n",
    "            'feature_names': self.feature_names\n",
    "        }\n",
    "        \n",
    "        with open(path, 'wb') as f:\n",
    "            import pickle\n",
    "            pickle.dump(model_data, f)\n",
    "\n",
    "    def load_model(self, path: str):\n",
    "        \"\"\"Load a trained model and associated data.\"\"\"\n",
    "        with open(path, 'rb') as f:\n",
    "            import pickle\n",
    "            model_data = pickle.load(f)\n",
    "            \n",
    "        self.model.load_model(model_data['model'])\n",
    "        self.scaler = model_data['scaler']\n",
    "        self.feature_names = model_data['feature_names']\n",
    "\n",
    "    def cross_validate(self, X: np.ndarray, y: np.ndarray, folds: int = 5) -> Dict[str, float]:\n",
    "        \"\"\"Perform cross-validation and return performance metrics.\"\"\"\n",
    "        from sklearn.model_selection import cross_validate\n",
    "        \n",
    "        metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "        scores = cross_validate(\n",
    "            self.model, \n",
    "            X, \n",
    "            y, \n",
    "            cv=folds,\n",
    "            scoring=metrics,\n",
    "            return_train_score=True\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            metric: float(scores[f'test_{metric}'].mean()) \n",
    "            for metric in metrics\n",
    "        }\n",
    "\n",
    "    def prepare_dataset(self, html_contents: List[str], labels: List[int]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Prepare features from multiple HTML contents for training or prediction.\n",
    "        \n",
    "        Args:\n",
    "            html_contents: List of HTML content strings\n",
    "            labels: List of corresponding labels (1 for phishing, 0 for legitimate)\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Array of extracted features for all HTML contents\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If input lengths don't match or if extraction fails\n",
    "        \"\"\"\n",
    "        if len(html_contents) != len(labels):\n",
    "            raise ValueError(\"Number of HTML contents must match number of labels\")\n",
    "            \n",
    "        features_list = []\n",
    "        failed_indices = []\n",
    "        \n",
    "        # Process each HTML content\n",
    "        for i, html in enumerate(html_contents):\n",
    "            try:\n",
    "                # Extract features for current HTML\n",
    "                features = self.extract_html_features(html)\n",
    "                features_list.append(features)\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Failed to extract features for sample {i}: {str(e)}\")\n",
    "                failed_indices.append(i)\n",
    "                continue\n",
    "        \n",
    "        if not features_list:\n",
    "            raise ValueError(\"Failed to extract features from all samples\")\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        features_array = np.array(features_list)\n",
    "        \n",
    "        # Initialize feature names if not already set\n",
    "        if not self.feature_names:\n",
    "            self.feature_names = [\n",
    "                # Basic form features\n",
    "                'form_count', 'password_fields', 'external_form_actions',\n",
    "                'suspicious_form_actions', 'input_field_count', 'sensitive_input_count',\n",
    "                'suspicious_input_names',\n",
    "                \n",
    "                # Link features\n",
    "                'link_count', 'external_links_ratio', 'suspicious_link_ratio',\n",
    "                'redirect_link_ratio', 'domain_diversity', 'max_domain_concentration',\n",
    "                'same_text_diff_targets',\n",
    "                \n",
    "                # Script features\n",
    "                'script_count', 'external_scripts', 'obfuscated_scripts',\n",
    "                'suspicious_functions', 'encoded_strings',\n",
    "                \n",
    "                # Content features\n",
    "                'text_length', 'urgent_word_count', 'suspicious_character_ratio',\n",
    "                'language_mix_score', 'hidden_element_count', 'suspicious_positioning',\n",
    "                \n",
    "                # Security features\n",
    "                'has_https_link', 'content-security-policy', 'x-frame-options',\n",
    "                'x-xss-protection', 'has_favicon', 'meta_tag_count',\n",
    "                \n",
    "                # Technical features\n",
    "                'tag_density', 'max_depth', 'resource_count', 'external_resource_ratio',\n",
    "                'data_uri_ratio', 'html_length', 'comments_length'\n",
    "            ]\n",
    "        \n",
    "        # Verify feature dimensions\n",
    "        if features_array.shape[1] != len(self.feature_names):\n",
    "            raise ValueError(\n",
    "                f\"Feature extraction produced {features_array.shape[1]} features, \"\n",
    "                f\"but expected {len(self.feature_names)}\"\n",
    "            )\n",
    "        \n",
    "        # Report on failed extractions\n",
    "        if failed_indices:\n",
    "            self.logger.warning(\n",
    "                f\"Failed to process {len(failed_indices)} samples at indices: {failed_indices}\"\n",
    "            )\n",
    "        \n",
    "        # Log feature statistics\n",
    "        self._log_feature_statistics(features_array)\n",
    "        \n",
    "        return features_array\n",
    "\n",
    "    def _log_feature_statistics(self, features: np.ndarray):\n",
    "        \"\"\"Log basic statistics about extracted features.\"\"\"\n",
    "        try:\n",
    "            feature_means = np.mean(features, axis=0)\n",
    "            feature_stds = np.std(features, axis=0)\n",
    "            \n",
    "            self.logger.info(\"Feature statistics:\")\n",
    "            for name, mean, std in zip(self.feature_names, feature_means, feature_stds):\n",
    "                self.logger.info(f\"{name}: mean={mean:.3f}, std={std:.3f}\")\n",
    "                \n",
    "            # Log potential issues\n",
    "            constant_features = np.where(feature_stds == 0)[0]\n",
    "            if len(constant_features) > 0:\n",
    "                self.logger.warning(\n",
    "                    \"Constant features detected: \" +\n",
    "                    \", \".join(self.feature_names[i] for i in constant_features)\n",
    "                )\n",
    "                \n",
    "            # Check for extreme values\n",
    "            max_values = np.max(features, axis=0)\n",
    "            min_values = np.min(features, axis=0)\n",
    "            for name, max_val, min_val in zip(self.feature_names, max_values, min_values):\n",
    "                if max_val > 1000 or min_val < -1000:\n",
    "                    self.logger.warning(\n",
    "                        f\"Extreme values detected for {name}: \"\n",
    "                        f\"min={min_val:.2f}, max={max_val:.2f}\"\n",
    "                    )\n",
    "                    \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error logging feature statistics: {str(e)}\")\n",
    "    def train(self, X: np.ndarray, y: np.ndarray) -> 'HTMLPhishingDetector':\n",
    "        \"\"\"\n",
    "        Обучение модели обнаружения фишинга с использованием CatBoost.\n",
    "        \n",
    "        Args:\n",
    "            X: np.ndarray - матрица признаков\n",
    "            y: np.ndarray - вектор меток (1 - фишинг, 0 - легитимный)\n",
    "            \n",
    "        Returns:\n",
    "            HTMLPhishingDetector: обученный экземпляр класса\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: если размерности данных неверны\n",
    "        \"\"\"\n",
    "        if X.shape[1] != len(self.feature_names):\n",
    "            raise ValueError(\n",
    "                f\"Ожидалось {len(self.feature_names)} признаков, \"\n",
    "                f\"получено {X.shape[1]}\"\n",
    "            )\n",
    "        \n",
    "        try:\n",
    "            # Масштабирование признаков\n",
    "            X_scaled = self.scaler.fit_transform(X)\n",
    "            \n",
    "            # Подготовка данных для обучения\n",
    "            train_pool = Pool(\n",
    "                data=X_scaled,\n",
    "                label=y,\n",
    "                feature_names=self.feature_names\n",
    "            )\n",
    "            \n",
    "            # Создание валидационного набора\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                X_scaled, y, \n",
    "                test_size=0.2, \n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            eval_pool = Pool(\n",
    "                data=X_val,\n",
    "                label=y_val,\n",
    "                feature_names=self.feature_names\n",
    "            )\n",
    "            \n",
    "            # Обучение модели CatBoost с мониторингом\n",
    "            self.model.fit(\n",
    "                train_pool,\n",
    "                eval_set=eval_pool,\n",
    "                plot=True,  # Построение графиков обучения\n",
    "                verbose=100  # Вывод прогресса каждые 100 итераций\n",
    "            )\n",
    "            \n",
    "            # Инициализация SHAP объяснений\n",
    "            try:\n",
    "                self.explainer = shap.TreeExplainer(self.model)\n",
    "                self.logger.info(\"SHAP объяснения успешно инициализированы\")\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Не удалось инициализировать SHAP: {str(e)}\")\n",
    "                self.explainer = None\n",
    "            \n",
    "            # Оценка качества на валидационном наборе\n",
    "            val_predictions = self.model.predict_proba(X_val)[:, 1]\n",
    "            val_auc = roc_auc_score(y_val, val_predictions)\n",
    "            self.logger.info(f\"Validation AUC: {val_auc:.3f}\")\n",
    "            \n",
    "            return self\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Ошибка при обучении модели: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def predict(self, html_content: str) -> Dict[str, Union[bool, float, str, List]]:\n",
    "        \"\"\"\n",
    "        Определение является ли HTML-страница фишинговой и объяснение причин.\n",
    "        \n",
    "        Args:\n",
    "            html_content: str - HTML-содержимое страницы\n",
    "            \n",
    "        Returns:\n",
    "            Dict с результатами:\n",
    "                - is_phishing: bool - является ли страница фишинговой\n",
    "                - confidence: float - уверенность предсказания\n",
    "                - explanation: str - объяснение на русском языке\n",
    "                - feature_importance: List - важнейшие признаки\n",
    "                - suspicious_elements: List - подозрительные элементы\n",
    "                - feature_values: Dict - значения признаков\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Извлечение признаков\n",
    "            features = self.extract_html_features(html_content)\n",
    "            features_scaled = self.scaler.transform(features.reshape(1, -1))\n",
    "            \n",
    "            # Получение предсказания и вероятностей\n",
    "            prediction = self.model.predict(features_scaled)[0]\n",
    "            probabilities = self.model.predict_proba(features_scaled)[0]\n",
    "            confidence = float(probabilities[1] if prediction == 1 else probabilities[0])\n",
    "            \n",
    "            # Генерация объяснений\n",
    "            explanation = \"Объяснение недоступно\"\n",
    "            feature_importance = []\n",
    "            \n",
    "            if self.explainer:\n",
    "                try:\n",
    "                    # Получение SHAP-значений\n",
    "                    shap_values = self.explainer.shap_values(features_scaled)\n",
    "                    if isinstance(shap_values, list):\n",
    "                        shap_vals = shap_values[1][0] if prediction == 1 else shap_values[0][0]\n",
    "                    else:\n",
    "                        shap_vals = shap_values[0]\n",
    "                    \n",
    "                    # Генерация объяснения и важности признаков\n",
    "                    explanation = self._generate_explanation(features, shap_vals)\n",
    "                    feature_importance = self._get_feature_importance(features, shap_vals)\n",
    "                except Exception as e:\n",
    "                    self.logger.warning(f\"Ошибка при генерации SHAP-объяснений: {str(e)}\")\n",
    "            \n",
    "            # Поиск подозрительных элементов\n",
    "            suspicious_elements = self._identify_suspicious_elements(html_content)\n",
    "            \n",
    "            # Формирование результата\n",
    "            result = {\n",
    "                'is_phishing': bool(prediction),\n",
    "                'confidence': confidence,\n",
    "                'explanation': explanation,\n",
    "                'feature_importance': feature_importance[:5],  # топ-5 важных признаков\n",
    "                'suspicious_elements': suspicious_elements,\n",
    "                'feature_values': dict(zip(self.feature_names, features))\n",
    "            }\n",
    "            \n",
    "            # Логирование результата\n",
    "            self.logger.info(\n",
    "                f\"Предсказание: {'фишинг' if prediction else 'легитимный'}, \"\n",
    "                f\"уверенность: {confidence:.2f}\"\n",
    "            )\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Ошибка при выполнении предсказания: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _generate_explanation(self, features: np.ndarray, shap_values: np.ndarray) -> str:\n",
    "        \"\"\"\n",
    "        Генерация понятного объяснения на русском языке на основе SHAP-значений.\n",
    "        \n",
    "        Args:\n",
    "            features: значения признаков\n",
    "            shap_values: SHAP-значения для признаков\n",
    "            \n",
    "        Returns:\n",
    "            str: текстовое объяснение на русском языке\n",
    "        \"\"\"\n",
    "        try:\n",
    "            feature_importance = self._get_feature_importance(features, shap_values)\n",
    "            explanations = []\n",
    "            \n",
    "            for feature, value, importance in feature_importance[:5]:\n",
    "                if abs(importance) < 0.1:  # игнорируем малозначимые признаки\n",
    "                    continue\n",
    "                \n",
    "                # Проверка различных признаков и формирование объяснений\n",
    "                if feature == 'external_links_ratio':\n",
    "                    explanations.append(\n",
    "                        f\"{'Подозрительно большое' if importance > 0 else 'Нормальное'} \"\n",
    "                        f\"количество внешних ссылок ({value:.1%})\"\n",
    "                    )\n",
    "                    \n",
    "                elif feature == 'suspicious_link_ratio' and value > 0:\n",
    "                    explanations.append(\n",
    "                        f\"Обнаружены подозрительные ссылки \"\n",
    "                        f\"(например, использование IP-адресов вместо доменных имён)\"\n",
    "                    )\n",
    "                    \n",
    "                elif feature == 'password_fields':\n",
    "                    explanations.append(\n",
    "                        f\"{'Подозрительное' if importance > 0 else 'Нормальное'} \"\n",
    "                        f\"количество полей для ввода пароля ({int(value)})\"\n",
    "                    )\n",
    "                    \n",
    "                elif feature == 'hidden_element_count':\n",
    "                    explanations.append(\n",
    "                        f\"{'Подозрительное' if importance > 0 else 'Нормальное'} \"\n",
    "                        f\"количество скрытых элементов на странице ({int(value)})\"\n",
    "                    )\n",
    "                    \n",
    "                elif feature == 'has_urgent_text' and value == 1:\n",
    "                    explanations.append(\n",
    "                        \"Страница содержит текст, создающий ложное чувство срочности \"\n",
    "                        \"или угрозы\"\n",
    "                    )\n",
    "                    \n",
    "                elif feature == 'sensitive_input_count':\n",
    "                    explanations.append(\n",
    "                        f\"{'Подозрительное' if importance > 0 else 'Нормальное'} \"\n",
    "                        f\"количество полей для ввода конфиденциальных данных ({int(value)})\"\n",
    "                    )\n",
    "                    \n",
    "                elif feature == 'obfuscated_scripts':\n",
    "                    explanations.append(\n",
    "                        f\"Обнаружен{'о' if value == 1 else 'ы'} {int(value)} \"\n",
    "                        f\"{'подозрительный скрипт' if value == 1 else 'подозрительных скриптов'} \"\n",
    "                        f\"с обфусцированным кодом\"\n",
    "                    )\n",
    "                    \n",
    "                elif feature == 'suspicious_positioning':\n",
    "                    explanations.append(\n",
    "                        f\"Обнаружено {int(value)} элементов с подозрительным \"\n",
    "                        f\"позиционированием на странице\"\n",
    "                    )\n",
    "                    \n",
    "                elif feature == 'domain_diversity':\n",
    "                    if importance > 0 and value > 0.5:\n",
    "                        explanations.append(\n",
    "                            \"Подозрительно большое разнообразие доменов в ссылках\"\n",
    "                        )\n",
    "                \n",
    "                elif feature == 'has_favicon' and value == 0:\n",
    "                    explanations.append(\n",
    "                        \"Отсутствует favicon, что нетипично для легитимных сайтов\"\n",
    "                    )\n",
    "            \n",
    "            if not explanations:\n",
    "                return \"Значительных подозрительных признаков не обнаружено.\"\n",
    "            \n",
    "            # Формирование итогового объяснения\n",
    "            main_explanation = \" и \".join(explanations) + \".\"\n",
    "            \n",
    "            # Добавление общего вывода\n",
    "            if len(explanations) > 2:\n",
    "                main_explanation += (\n",
    "                    \"\\nСовокупность этих признаков характерна для фишинговых страниц.\"\n",
    "                )\n",
    "            \n",
    "            return main_explanation\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Ошибка при генерации объяснения: {str(e)}\")\n",
    "            return \"Не удалось сгенерировать объяснение из-за технической ошибки.\"\n",
    "    def _get_feature_importance(\n",
    "            self, \n",
    "            features: np.ndarray, \n",
    "            shap_values: np.ndarray\n",
    "        ) -> List[Tuple[str, float, float]]:\n",
    "        \"\"\"\n",
    "        Расчёт важности признаков с использованием SHAP-значений.\n",
    "        \n",
    "        Args:\n",
    "            features: значения признаков\n",
    "            shap_values: SHAP-значения\n",
    "            \n",
    "        Returns:\n",
    "            List[Tuple]: список кортежей (имя признака, значение, важность)\n",
    "        \"\"\"\n",
    "        feature_importance = []\n",
    "        \n",
    "        for name, value, importance in zip(self.feature_names, features, shap_values):\n",
    "            # Приведение к обычным Python-типам из numpy\n",
    "            value = float(np.asarray(value).item() if hasattr(value, 'item') else value)\n",
    "            importance = float(np.asarray(importance).item() if hasattr(importance, 'item') else importance)\n",
    "            feature_importance.append((name, value, importance))\n",
    "        \n",
    "        # Сортировка по абсолютному значению важности\n",
    "        return sorted(feature_importance, key=lambda x: abs(x[2]), reverse=True)\n",
    "\n",
    "    def _identify_suspicious_elements(self, html_content: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Поиск подозрительных элементов в HTML-содержимом.\n",
    "        \n",
    "        Args:\n",
    "            html_content: HTML-содержимое\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: список подозрительных элементов с их описанием\n",
    "        \"\"\"\n",
    "        try:\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            suspicious = []\n",
    "            \n",
    "            # Проверка форм с внешними действиями\n",
    "            for form in soup.find_all('form'):\n",
    "                action = form.get('action', '')\n",
    "                if action.startswith(('http', '//')):\n",
    "                    suspicious.append({\n",
    "                        'type': 'form',\n",
    "                        'issue': 'Форма отправляет данные на внешний сайт',\n",
    "                        'element': str(form)[:200] + '...' if len(str(form)) > 200 else str(form)\n",
    "                    })\n",
    "\n",
    "            # Проверка подозрительных скриптов\n",
    "            scripts = soup.find_all('script')\n",
    "            for script in scripts:\n",
    "                content = script.string if script.string else ''\n",
    "                if content and self.patterns['obfuscated_js'].search(content):\n",
    "                    suspicious.append({\n",
    "                        'type': 'script',\n",
    "                        'issue': 'Обнаружен обфусцированный JavaScript-код',\n",
    "                        'element': str(script)[:200] + '...' if len(str(script)) > 200 else str(script)\n",
    "                    })\n",
    "\n",
    "            # Проверка подозрительных iframe\n",
    "            iframes = soup.find_all('iframe')\n",
    "            for iframe in iframes:\n",
    "                src = iframe.get('src', '')\n",
    "                if src.startswith(('http', '//')) or not src:\n",
    "                    suspicious.append({\n",
    "                        'type': 'iframe',\n",
    "                        'issue': 'Подозрительное использование iframe',\n",
    "                        'element': str(iframe)\n",
    "                    })\n",
    "\n",
    "            # Проверка data URI в ссылках или формах\n",
    "            elements_with_uri = soup.find_all(['a', 'form', 'img'], href=re.compile(r'^data:'))\n",
    "            for element in elements_with_uri:\n",
    "                suspicious.append({\n",
    "                    'type': element.name,\n",
    "                    'issue': 'Использование подозрительного data URI',\n",
    "                    'element': str(element)\n",
    "                })\n",
    "\n",
    "            # Проверка скрытых элементов\n",
    "            hidden_elements = soup.find_all(\n",
    "                ['div', 'span', 'input', 'p'], \n",
    "                style=re.compile(r'display\\s*:\\s*none|visibility\\s*:\\s*hidden', re.I)\n",
    "            )\n",
    "            for element in hidden_elements:\n",
    "                if element.find(['form', 'input', 'select']):\n",
    "                    suspicious.append({\n",
    "                        'type': 'hidden_element',\n",
    "                        'issue': 'Скрытая форма или поле ввода',\n",
    "                        'element': str(element)[:200] + '...' if len(str(element)) > 200 else str(element)\n",
    "                    })\n",
    "\n",
    "            # Проверка элементов, расположенных за пределами видимой области\n",
    "            positioned_elements = soup.find_all(\n",
    "                style=re.compile(r'position\\s*:\\s*absolute|position\\s*:\\s*fixed')\n",
    "            )\n",
    "            for element in positioned_elements:\n",
    "                style = element.get('style', '')\n",
    "                if re.search(r'(?:top|left|right|bottom)\\s*:\\s*-?\\d{4,}px', style):\n",
    "                    suspicious.append({\n",
    "                        'type': 'positioning',\n",
    "                        'issue': 'Элемент расположен за пределами видимой области',\n",
    "                        'element': str(element)[:200] + '...' if len(str(element)) > 200 else str(element)\n",
    "                    })\n",
    "\n",
    "            # Проверка смешанного контента (HTTP на HTTPS)\n",
    "            if soup.find('meta', {'content': re.compile('https')}):\n",
    "                mixed_content = soup.find_all(\n",
    "                    ['img', 'script', 'link', 'iframe'], \n",
    "                    src=re.compile(r'^http:\\/\\/')\n",
    "                )\n",
    "                for element in mixed_content:\n",
    "                    suspicious.append({\n",
    "                        'type': element.name,\n",
    "                        'issue': 'Небезопасное содержимое (HTTP на HTTPS)',\n",
    "                        'element': str(element)\n",
    "                    })\n",
    "\n",
    "            # Проверка подозрительных редиректов\n",
    "            for a in soup.find_all('a'):\n",
    "                href = a.get('href', '')\n",
    "                if any(pattern in href.lower() for pattern in self.known_patterns['common_redirects']):\n",
    "                    suspicious.append({\n",
    "                        'type': 'redirect',\n",
    "                        'issue': 'Подозрительный редирект',\n",
    "                        'element': str(a)\n",
    "                    })\n",
    "\n",
    "            return suspicious\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Ошибка при поиске подозрительных элементов: {str(e)}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = json.load(open('webs.json', 'r', encoding='utf-8'))[:12000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils = DatasetUtils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_contents = [item['text'] for item in data]\n",
    "labels = [item['label'] for item in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize detector\n",
    "detector = HTMLPhishingDetector(\n",
    "    iterations=2000,  # Number of boosting iterations\n",
    "    learning_rate=0.1,  # Learning rate\n",
    "    random_state=42  # Random seed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-01 17:46:18,706 - __main__ - INFO - Feature statistics:\n",
      "2024-12-01 17:46:18,707 - __main__ - INFO - form_count: mean=1.139, std=1.602\n",
      "2024-12-01 17:46:18,707 - __main__ - INFO - password_fields: mean=0.312, std=0.804\n",
      "2024-12-01 17:46:18,708 - __main__ - INFO - external_form_actions: mean=0.355, std=0.850\n",
      "2024-12-01 17:46:18,708 - __main__ - INFO - suspicious_form_actions: mean=0.000, std=0.000\n",
      "2024-12-01 17:46:18,708 - __main__ - INFO - input_field_count: mean=5.505, std=9.526\n",
      "2024-12-01 17:46:18,709 - __main__ - INFO - sensitive_input_count: mean=0.667, std=1.387\n",
      "2024-12-01 17:46:18,709 - __main__ - INFO - suspicious_input_names: mean=0.233, std=0.674\n",
      "2024-12-01 17:46:18,709 - __main__ - INFO - link_count: mean=47.209, std=60.361\n",
      "2024-12-01 17:46:18,710 - __main__ - INFO - external_links_ratio: mean=0.391, std=0.374\n",
      "2024-12-01 17:46:18,710 - __main__ - INFO - suspicious_link_ratio: mean=0.000, std=0.000\n",
      "2024-12-01 17:46:18,710 - __main__ - INFO - redirect_link_ratio: mean=0.008, std=0.041\n",
      "2024-12-01 17:46:18,711 - __main__ - INFO - domain_diversity: mean=0.000, std=0.000\n",
      "2024-12-01 17:46:18,711 - __main__ - INFO - max_domain_concentration: mean=0.000, std=0.000\n",
      "2024-12-01 17:46:18,711 - __main__ - INFO - same_text_diff_targets: mean=2.563, std=8.852\n",
      "2024-12-01 17:46:18,712 - __main__ - INFO - script_count: mean=11.851, std=12.450\n",
      "2024-12-01 17:46:18,712 - __main__ - INFO - external_scripts: mean=3.995, std=6.873\n",
      "2024-12-01 17:46:18,712 - __main__ - INFO - obfuscated_scripts: mean=0.472, std=1.144\n",
      "2024-12-01 17:46:18,713 - __main__ - INFO - suspicious_functions: mean=0.711, std=1.246\n",
      "2024-12-01 17:46:18,713 - __main__ - INFO - encoded_strings: mean=1.464, std=9.496\n",
      "2024-12-01 17:46:18,713 - __main__ - INFO - text_length: mean=3222.124, std=4769.616\n",
      "2024-12-01 17:46:18,714 - __main__ - INFO - urgent_word_count: mean=5.431, std=11.915\n",
      "2024-12-01 17:46:18,714 - __main__ - INFO - suspicious_character_ratio: mean=0.030, std=0.119\n",
      "2024-12-01 17:46:18,714 - __main__ - INFO - language_mix_score: mean=0.021, std=0.097\n",
      "2024-12-01 17:46:18,715 - __main__ - INFO - hidden_element_count: mean=1.021, std=3.612\n",
      "2024-12-01 17:46:18,715 - __main__ - INFO - suspicious_positioning: mean=0.160, std=1.843\n",
      "2024-12-01 17:46:18,715 - __main__ - INFO - has_https_link: mean=0.635, std=0.481\n",
      "2024-12-01 17:46:18,715 - __main__ - INFO - content-security-policy: mean=8.910, std=10.745\n",
      "2024-12-01 17:46:18,716 - __main__ - INFO - x-frame-options: mean=0.005, std=0.071\n",
      "2024-12-01 17:46:18,716 - __main__ - INFO - x-xss-protection: mean=0.000, std=0.016\n",
      "2024-12-01 17:46:18,717 - __main__ - INFO - has_favicon: mean=0.001, std=0.032\n",
      "2024-12-01 17:46:18,717 - __main__ - INFO - meta_tag_count: mean=0.649, std=0.477\n",
      "2024-12-01 17:46:18,717 - __main__ - INFO - tag_density: mean=0.012, std=0.008\n",
      "2024-12-01 17:46:18,717 - __main__ - INFO - max_depth: mean=27.698, std=38.086\n",
      "2024-12-01 17:46:18,718 - __main__ - INFO - resource_count: mean=29.690, std=28.815\n",
      "2024-12-01 17:46:18,718 - __main__ - INFO - external_resource_ratio: mean=0.342, std=0.309\n",
      "2024-12-01 17:46:18,718 - __main__ - INFO - data_uri_ratio: mean=0.007, std=0.044\n",
      "2024-12-01 17:46:18,719 - __main__ - INFO - html_length: mean=28083.854, std=24117.267\n",
      "2024-12-01 17:46:18,719 - __main__ - INFO - comments_length: mean=12831.493, std=15859.982\n",
      "2024-12-01 17:46:18,719 - __main__ - WARNING - Constant features detected: suspicious_form_actions, suspicious_link_ratio, domain_diversity, max_domain_concentration\n",
      "2024-12-01 17:46:18,720 - __main__ - WARNING - Extreme values detected for text_length: min=0.00, max=80157.00\n",
      "2024-12-01 17:46:18,720 - __main__ - WARNING - Extreme values detected for max_depth: min=1.00, max=1318.00\n",
      "2024-12-01 17:46:18,720 - __main__ - WARNING - Extreme values detected for html_length: min=6.00, max=101542.00\n",
      "2024-12-01 17:46:18,720 - __main__ - WARNING - Extreme values detected for comments_length: min=0.00, max=100960.00\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset\n",
    "X = detector.prepare_dataset(html_contents, labels)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelPersistence:\n",
    "    @staticmethod\n",
    "    def save_model(detector: 'HTMLPhishingDetector', filepath: str) -> bool:\n",
    "        \"\"\"\n",
    "        Save the trained model and its components to a file.\n",
    "        \n",
    "        Args:\n",
    "            detector: Trained HTMLPhishingDetector instance\n",
    "            filepath: Path where to save the model\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if save was successful\n",
    "        \"\"\"\n",
    "        try:\n",
    "            model_state = {\n",
    "                'model': detector.model,\n",
    "                'scaler': detector.scaler,\n",
    "                'feature_names': detector.feature_names,\n",
    "                'explainer': detector.explainer\n",
    "            }\n",
    "            \n",
    "            os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "            with open(filepath, 'wb') as f:\n",
    "                pickle.dump(model_state, f)\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            detector.logger.error(f\"Error saving model: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(filepath: str) -> 'HTMLPhishingDetector':\n",
    "        \"\"\"\n",
    "        Load a saved model and its components.\n",
    "        \n",
    "        Args:\n",
    "            filepath: Path to the saved model file\n",
    "            \n",
    "        Returns:\n",
    "            HTMLPhishingDetector: Loaded model instance\n",
    "            \n",
    "        Raises:\n",
    "            FileNotFoundError: If model file doesn't exist\n",
    "        \"\"\"\n",
    "        if not os.path.exists(filepath):\n",
    "            raise FileNotFoundError(f\"Model file not found: {filepath}\")\n",
    "            \n",
    "        try:\n",
    "            with open(filepath, 'rb') as f:\n",
    "                model_state = pickle.load(f)\n",
    "                \n",
    "            detector = HTMLPhishingDetector()\n",
    "            detector.model = model_state['model']\n",
    "            detector.scaler = model_state['scaler']\n",
    "            detector.feature_names = model_state['feature_names']\n",
    "            detector.explainer = model_state['explainer']\n",
    "            \n",
    "            return detector\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading model: {str(e)}\")\n",
    "\n",
    "# Add model persistence methods to HTMLPhishingDetector\n",
    "def save(self, filepath: str) -> bool:\n",
    "    \"\"\"Save the model to a file.\"\"\"\n",
    "    return ModelPersistence.save_model(self, filepath)\n",
    "\n",
    "def load(cls, filepath: str) -> 'HTMLPhishingDetector':\n",
    "    \"\"\"Load the model from a file.\"\"\"\n",
    "    return ModelPersistence.load_model(filepath)\n",
    "\n",
    "# Add these methods to HTMLPhishingDetector class\n",
    "HTMLPhishingDetector.save = save\n",
    "HTMLPhishingDetector.load = classmethod(load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Dataset saved successfully to ./prepared_dataset.pkl\n",
      "INFO:__main__:Dataset shape: (10000, 20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save dataset\n",
    "save_path = \"./prepared_dataset.pkl\"\n",
    "metadata = {\n",
    "    'description': 'HTML phishing detection features',\n",
    "        'num_samples': len(labels),\n",
    "        'positive_samples': sum(labels)\n",
    "    }\n",
    "utils.save_prepared_dataset(\n",
    "        X=X,\n",
    "        y=y,\n",
    "        feature_names=detector.feature_names,\n",
    "        filepath=save_path,\n",
    "    metadata=metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Dataset loaded successfully from ./prepared_dataset.pkl\n",
      "INFO:__main__:Dataset shape: (10000, 20)\n"
     ]
    }
   ],
   "source": [
    "# Later, load the dataset\n",
    "X_loaded, y_loaded, feature_names, metadata = utils.load_prepared_dataset(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "def example_usage():\n",
    "    # Initialize utilities\n",
    "    \n",
    "    \n",
    "    # Prepare your dataset (example)\n",
    "    detector = HTMLPhishingDetector()\n",
    "    X = detector.prepare_dataset(html_contents, labels)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    # For large datasets, use batch processing\n",
    "    batch_files = utils.save_features_batch(\n",
    "        features_list=X.tolist(),\n",
    "        batch_size=1000,\n",
    "        output_dir='feature_batches'\n",
    "    )\n",
    "    \n",
    "    # Load batches when needed\n",
    "    X_combined = utils.load_features_batch(batch_files)\n",
    "    \n",
    "    # Verify dataset integrity\n",
    "    is_valid = utils.verify_dataset_integrity(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39eb8108684b4113ba973977a416b843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.8866268\tbest: 0.8866268 (0)\ttotal: 58.4ms\tremaining: 1m 56s\n",
      "100:\ttest: 0.9726499\tbest: 0.9726499 (100)\ttotal: 370ms\tremaining: 6.96s\n",
      "200:\ttest: 0.9857806\tbest: 0.9857806 (200)\ttotal: 676ms\tremaining: 6.05s\n",
      "300:\ttest: 0.9916114\tbest: 0.9916114 (300)\ttotal: 969ms\tremaining: 5.47s\n",
      "400:\ttest: 0.9948724\tbest: 0.9948739 (399)\ttotal: 1.25s\tremaining: 5s\n",
      "500:\ttest: 0.9969490\tbest: 0.9969490 (500)\ttotal: 1.54s\tremaining: 4.62s\n",
      "600:\ttest: 0.9986647\tbest: 0.9986647 (600)\ttotal: 1.82s\tremaining: 4.25s\n",
      "700:\ttest: 0.9992819\tbest: 0.9992819 (700)\ttotal: 2.12s\tremaining: 3.94s\n",
      "800:\ttest: 0.9996107\tbest: 0.9996107 (800)\ttotal: 2.43s\tremaining: 3.63s\n",
      "900:\ttest: 0.9997863\tbest: 0.9997863 (898)\ttotal: 2.71s\tremaining: 3.31s\n",
      "1000:\ttest: 0.9998804\tbest: 0.9998804 (999)\ttotal: 3.01s\tremaining: 3s\n",
      "1100:\ttest: 0.9999141\tbest: 0.9999156 (1094)\ttotal: 3.3s\tremaining: 2.69s\n",
      "1200:\ttest: 0.9999357\tbest: 0.9999357 (1199)\ttotal: 3.58s\tremaining: 2.38s\n",
      "1300:\ttest: 0.9999402\tbest: 0.9999410 (1263)\ttotal: 3.87s\tremaining: 2.08s\n",
      "1400:\ttest: 0.9999544\tbest: 0.9999544 (1377)\ttotal: 4.16s\tremaining: 1.78s\n",
      "1500:\ttest: 0.9999596\tbest: 0.9999611 (1469)\ttotal: 4.44s\tremaining: 1.48s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.999961143\n",
      "bestIteration = 1469\n",
      "\n",
      "Shrink model to first 1470 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-01 17:46:45,904 - __main__ - INFO - SHAP объяснения успешно инициализированы\n",
      "2024-12-01 17:46:45,922 - __main__ - INFO - Validation AUC: 1.000\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'HTMLPhishingDetector' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trained \u001b[38;5;241m=\u001b[39m detector\u001b[38;5;241m.\u001b[39mtrain(X, y)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrained\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/phishing_detector1.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'HTMLPhishingDetector' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "trained = detector.train(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained.save(\"models/phishing_detector1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = HTMLPhishingDetector.load(\"models/phishing_detector1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector.save(\"models/phishing_detector1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "alll =json.load(open('webs.json', 'r', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15756"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_sample():\n",
    "    rng = np.random.default_rng()\n",
    "    idx = rng.integers(0, len(alll))\n",
    "    return alll[idx]['text'], alll[idx]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-01 17:49:00,790 - __main__ - WARNING - Ошибка при генерации объяснения: 'HTMLPhishingDetector' object has no attribute '_get_feature_importance'\n",
      "2024-12-01 17:49:00,791 - __main__ - WARNING - Ошибка при генерации SHAP-объяснений: 'HTMLPhishingDetector' object has no attribute '_get_feature_importance'\n",
      "2024-12-01 17:49:00,791 - __main__ - ERROR - Ошибка при выполнении предсказания: 'HTMLPhishingDetector' object has no attribute '_identify_suspicious_elements'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'HTMLPhishingDetector' object has no attribute '_identify_suspicious_elements'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sample_html, true_label \u001b[38;5;241m=\u001b[39m get_random_sample()\n\u001b[0;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_html\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIs phishing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_phishing\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfidence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[34], line 746\u001b[0m, in \u001b[0;36mHTMLPhishingDetector.predict\u001b[0;34m(self, html_content)\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mОшибка при генерации SHAP-объяснений: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    745\u001b[0m \u001b[38;5;66;03m# Поиск подозрительных элементов\u001b[39;00m\n\u001b[0;32m--> 746\u001b[0m suspicious_elements \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_identify_suspicious_elements\u001b[49m(html_content)\n\u001b[1;32m    748\u001b[0m \u001b[38;5;66;03m# Формирование результата\u001b[39;00m\n\u001b[1;32m    749\u001b[0m result \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    750\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_phishing\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mbool\u001b[39m(prediction),\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m'\u001b[39m: confidence,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_values\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names, features))\n\u001b[1;32m    756\u001b[0m }\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'HTMLPhishingDetector' object has no attribute '_identify_suspicious_elements'"
     ]
    }
   ],
   "source": [
    "sample_html, true_label = get_random_sample()\n",
    "result = detector.predict(sample_html)\n",
    "print(f\"Is phishing: {result['is_phishing']}\")\n",
    "print(f\"Confidence: {result['confidence']:.2f}\")\n",
    "print(f\"Explanation: {result['explanation']}\")\n",
    "print(f\"Suspicious elements: {result['suspicious_elements']}\")\n",
    "print(f\"True label: {'Phishing' if true_label == 1 else 'Legitimate'}\")\n",
    "print(f\"Is model correct: {result['is_phishing'] == (true_label == 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(detector, n_tests=100):\n",
    "    correct = 0\n",
    "    for i in range(n_tests):\n",
    "        sample_html, true_label = get_random_sample()\n",
    "        result = detector.predict(sample_html)\n",
    "        correct += result['is_phishing'] == (true_label == 1)\n",
    "    return correct / n_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.961"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(detector, n_tests=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_phishing': False,\n",
       " 'confidence': 0.9999114840223284,\n",
       " 'explanation': 'The page contains urgent or threatening language.',\n",
       " 'feature_importance': [('text_length', 38640.0, -3.0330556120506316),\n",
       "  ('link_count', 33.0, -1.1929234857212316),\n",
       "  ('meta_tag_count', 39.0, -1.18454137311926),\n",
       "  ('has_urgent_text', 1.0, -0.7294605416411305),\n",
       "  ('has_favicon', 0.0, -0.6479692336794542)],\n",
       " 'suspicious_elements': [],\n",
       " 'feature_values': {'form_count': 0.0,\n",
       "  'password_fields': 0.0,\n",
       "  'external_form_actions': 0.0,\n",
       "  'link_count': 33.0,\n",
       "  'external_links_ratio': 0.9615384615384616,\n",
       "  'suspicious_link_ratio': 0.0,\n",
       "  'script_count': 5.0,\n",
       "  'external_scripts': 2.0,\n",
       "  'meta_tag_count': 39.0,\n",
       "  'has_favicon': 0.0,\n",
       "  'has_title': 1.0,\n",
       "  'title_length': 33.0,\n",
       "  'hidden_element_count': 0.0,\n",
       "  'has_https_link': 1.0,\n",
       "  'has_security_text': 1.0,\n",
       "  'text_length': 38640.0,\n",
       "  'has_urgent_text': 1.0,\n",
       "  'input_field_count': 0.0,\n",
       "  'sensitive_input_count': 0.0,\n",
       "  'has_submit_button': 0.0}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phishy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
